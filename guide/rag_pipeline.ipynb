{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "244802d1-2b79-4d36-af0b-c5ae523f828b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:25:20.264816Z",
     "iopub.status.busy": "2024-12-16T17:25:20.263299Z",
     "iopub.status.idle": "2024-12-16T17:25:22.945651Z",
     "shell.execute_reply": "2024-12-16T17:25:22.944272Z",
     "shell.execute_reply.started": "2024-12-16T17:25:20.264740Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58755c66-66d8-4417-aed3-b17b2b6190f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:25:22.949473Z",
     "iopub.status.busy": "2024-12-16T17:25:22.947582Z",
     "iopub.status.idle": "2024-12-16T17:25:23.050964Z",
     "shell.execute_reply": "2024-12-16T17:25:23.049705Z",
     "shell.execute_reply.started": "2024-12-16T17:25:22.949400Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/jupyter/datasphere/project/mipt_hackathon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d799ec51-2531-45b3-8fd9-3e70e65ef529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:25:23.052861Z",
     "iopub.status.busy": "2024-12-16T17:25:23.052187Z",
     "iopub.status.idle": "2024-12-16T17:25:23.122151Z",
     "shell.execute_reply": "2024-12-16T17:25:23.120864Z",
     "shell.execute_reply.started": "2024-12-16T17:25:23.052807Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WikiData</th>\n",
       "      <th>wiki_links</th>\n",
       "      <th>История</th>\n",
       "      <th>Архитектура</th>\n",
       "      <th>llm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q37996725</td>\n",
       "      <td>https://ru.wikipedia.org/wiki/%D0%94%D0%B8%D0%...</td>\n",
       "      <td>В сентябре 1929 годаЦК ВКП(б)было принято пост...</td>\n",
       "      <td>Здание спортклуба, напоминающее корабль, имеет...</td>\n",
       "      <td>### История и Архитектура Здания Спортклуба\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q55209768</td>\n",
       "      <td>https://ru.wikipedia.org/wiki/%D0%94%D0%BE%D0%...</td>\n",
       "      <td>Комплекс зданийДОСААФбыл расположен на месте т...</td>\n",
       "      <td>Здания клуба, спортивного техникума и жилого д...</td>\n",
       "      <td>### История\\n\\nКлуб \"Малышева\" - это уникально...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q55154121</td>\n",
       "      <td>https://ru.wikipedia.org/wiki/%D0%93%D0%BE%D1%...</td>\n",
       "      <td>В 1894 году выходцы из купеческих семей Андрей...</td>\n",
       "      <td>Комплекс зданий электростанции расположен в це...</td>\n",
       "      <td>Конечно! Вот краткий обзор текста:\\n\\n**Истори...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q55232375</td>\n",
       "      <td>https://ru.wikipedia.org/wiki/%D0%94%D0%BE%D0%...</td>\n",
       "      <td>Автором проекта являлся городской архитектор Е...</td>\n",
       "      <td>Одноэтажное кирпичное здание с цокольным этажо...</td>\n",
       "      <td>### История и Архитектура Здания\\n\\n**Здание:*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q4306077</td>\n",
       "      <td>https://ru.wikipedia.org/wiki/%D0%9B%D0%B8%D1%...</td>\n",
       "      <td>В 1910—1912 годах дом, в котором теперь распол...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Конечно, я помогу вам создать краткое описание...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    WikiData  ...                                           llm_text\n",
       "0  Q37996725  ...  ### История и Архитектура Здания Спортклуба\\n\\...\n",
       "1  Q55209768  ...  ### История\\n\\nКлуб \"Малышева\" - это уникально...\n",
       "2  Q55154121  ...  Конечно! Вот краткий обзор текста:\\n\\n**Истори...\n",
       "3  Q55232375  ...  ### История и Архитектура Здания\\n\\n**Здание:*...\n",
       "4   Q4306077  ...  Конечно, я помогу вам создать краткое описание...\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d2e798-74e2-4b53-8b93-835468e81dff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:25:23.125969Z",
     "iopub.status.busy": "2024-12-16T17:25:23.124426Z",
     "iopub.status.idle": "2024-12-16T17:25:23.211362Z",
     "shell.execute_reply": "2024-12-16T17:25:23.210055Z",
     "shell.execute_reply.started": "2024-12-16T17:25:23.125906Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b00db129-f759-4217-85e6-0d1f3680f415",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:25:23.214250Z",
     "iopub.status.busy": "2024-12-16T17:25:23.212583Z",
     "iopub.status.idle": "2024-12-16T17:25:24.592325Z",
     "shell.execute_reply": "2024-12-16T17:25:24.591097Z",
     "shell.execute_reply.started": "2024-12-16T17:25:23.214149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafde5e135cc4abd8423c913676a868e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"llm_text\"], metadata={\"source\": doc[\"WikiData\"]}) for doc in tqdm(ds)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21ff96f4-929a-4041-95fc-7316f922373f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:25:24.594741Z",
     "iopub.status.busy": "2024-12-16T17:25:24.593823Z",
     "iopub.status.idle": "2024-12-16T17:25:24.688983Z",
     "shell.execute_reply": "2024-12-16T17:25:24.687656Z",
     "shell.execute_reply.started": "2024-12-16T17:25:24.594684Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93d8464c-7788-4a62-9c32-ded40d6e502d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:25:24.691059Z",
     "iopub.status.busy": "2024-12-16T17:25:24.690197Z",
     "iopub.status.idle": "2024-12-16T17:25:24.706353Z",
     "shell.execute_reply": "2024-12-16T17:25:24.705188Z",
     "shell.execute_reply.started": "2024-12-16T17:25:24.691007Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = 'intfloat/multilingual-e5-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d83639ad-2cca-4f95-8135-ed95ba4d1a61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:25:24.709555Z",
     "iopub.status.busy": "2024-12-16T17:25:24.707879Z",
     "iopub.status.idle": "2024-12-16T17:25:43.452112Z",
     "shell.execute_reply": "2024-12-16T17:25:43.450714Z",
     "shell.execute_reply.started": "2024-12-16T17:25:24.709499Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.8/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Разобъём документы на блоки максимального размера `chunk_size` и вернём список документов.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=MARKDOWN_SEPARATORS,\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Удалим дубли\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique\n",
    "\n",
    "\n",
    "docs_processed = split_documents(\n",
    "    1024,  # Выбираем размер чанка, соответствующий модели\n",
    "    RAW_KNOWLEDGE_BASE,\n",
    "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8002ca2f-e716-4f96-8df5-230aed3fa0ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:25:43.454510Z",
     "iopub.status.busy": "2024-12-16T17:25:43.453625Z",
     "iopub.status.idle": "2024-12-16T17:27:11.354746Z",
     "shell.execute_reply": "2024-12-16T17:27:11.353350Z",
     "shell.execute_reply.started": "2024-12-16T17:25:43.454468Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # `True` для косинусного сходства\n",
    ")\n",
    "\n",
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e298e5c4-7a24-4e53-b5cc-e02e615bc4fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:27:11.361148Z",
     "iopub.status.busy": "2024-12-16T17:27:11.360169Z",
     "iopub.status.idle": "2024-12-16T17:27:11.386407Z",
     "shell.execute_reply": "2024-12-16T17:27:11.384855Z",
     "shell.execute_reply.started": "2024-12-16T17:27:11.361106Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'VityaVitalich/Llama3.1-8b-instruct'\n",
    "# model_name = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "# model_name = 'intfloat/multilingual-e5-small'\n",
    "# model_name = \"BAAI/bge-multilingual-gemma2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3212758d-6c48-401b-b6ef-bd04344b1378",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:28:01.655403Z",
     "iopub.status.busy": "2024-12-16T17:28:01.654428Z",
     "iopub.status.idle": "2024-12-16T17:33:51.329400Z",
     "shell.execute_reply": "2024-12-16T17:33:51.328139Z",
     "shell.execute_reply.started": "2024-12-16T17:28:01.655356Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 391.71it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [05:44<00:00, 86.19s/it] \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quant = True\n",
    "\n",
    "bnb_config = None\n",
    "if quant:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_quant_type=\"nf4\",\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "  )\n",
    "    \n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map='cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7af09170-6d78-4aca-91b2-f75bdebd8565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:33:51.333279Z",
     "iopub.status.busy": "2024-12-16T17:33:51.331248Z",
     "iopub.status.idle": "2024-12-16T17:33:51.347907Z",
     "shell.execute_reply": "2024-12-16T17:33:51.346777Z",
     "shell.execute_reply.started": "2024-12-16T17:33:51.333238Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "READER_LLM = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cfa962b-08c5-4855-83a0-3d5d37c1c0e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:33:51.350044Z",
     "iopub.status.busy": "2024-12-16T17:33:51.349285Z",
     "iopub.status.idle": "2024-12-16T17:33:51.381773Z",
     "shell.execute_reply": "2024-12-16T17:33:51.380530Z",
     "shell.execute_reply.started": "2024-12-16T17:33:51.349987Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Используя информацию, содержащуюся в контексте, \n",
      "        предоставьте подробный и содержательный ответ на заданный вопрос. \n",
      "        Убедитесь, что ваш ответ охватывает ключевые аспекты темы и включает примеры или пояснения для лучшего понимания. \n",
      "        Если контекст не содержит необходимой информации для ответа, \n",
      "        используйте свои знания по сути вопроса и дайте обоснованный ответ. \n",
      "        Постарайтесь сделать вашу речь живой и увлекательной, \n",
      "        чтобы заинтересовать читателя. Отвечайте на русском языке.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Контекст:\n",
      "{context}\n",
      "---\n",
      "Теперь вот вопрос, на который вам нужно ответить.\n",
      "\n",
      "Вопрос: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_in_chat_format = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Используя информацию, содержащуюся в контексте, \n",
    "        предоставьте подробный и содержательный ответ на заданный вопрос. \n",
    "        Убедитесь, что ваш ответ охватывает ключевые аспекты темы и включает примеры или пояснения для лучшего понимания. \n",
    "        Если контекст не содержит необходимой информации для ответа, \n",
    "        используйте свои знания по сути вопроса и дайте обоснованный ответ. \n",
    "        Постарайтесь сделать вашу речь живой и увлекательной, \n",
    "        чтобы заинтересовать читателя. Отвечайте на русском языке.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Контекст:\n",
    "{context}\n",
    "---\n",
    "Теперь вот вопрос, на который вам нужно ответить.\n",
    "\n",
    "Вопрос: {question}\"\"\",\n",
    "    },\n",
    "]\n",
    "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
    "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(RAG_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7166eb7-cb60-4b92-9768-56196ce6e94f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:33:56.682430Z",
     "iopub.status.busy": "2024-12-16T17:33:56.680941Z",
     "iopub.status.idle": "2024-12-16T17:33:56.704815Z",
     "shell.execute_reply": "2024-12-16T17:33:56.703563Z",
     "shell.execute_reply.started": "2024-12-16T17:33:56.682381Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Pipeline\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: Pipeline,\n",
    "    knowledge_index: FAISS,\n",
    "    reranker = False,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 5,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    # Соберём документы с помощью ретривера\n",
    "    print(\"=> Retrieving documents...\")\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # Оставляем только текст\n",
    "\n",
    "    if reranker:\n",
    "        print(\"=> Reranking documents...\")\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Финальный промпт\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    print(\"=> Generating answer...\")\n",
    "    answer = llm(final_prompt, pad_token_id=llm.tokenizer.eos_token_id)[0][\"generated_text\"]\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc632216-ef51-4759-bd56-84d00d1ad4f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:36:00.275270Z",
     "iopub.status.busy": "2024-12-16T17:36:00.273748Z",
     "iopub.status.idle": "2024-12-16T17:37:00.317798Z",
     "shell.execute_reply": "2024-12-16T17:37:00.316300Z",
     "shell.execute_reply.started": "2024-12-16T17:36:00.275229Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating answer...\n"
     ]
    }
   ],
   "source": [
    "question = \"Расскажи про Здание спортклуба\"\n",
    "\n",
    "answer = answer_with_rag(question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE, reranker=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa1f5e68-b837-44d0-88b7-f720afa5d135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:37:00.320641Z",
     "iopub.status.busy": "2024-12-16T17:37:00.319885Z",
     "iopub.status.idle": "2024-12-16T17:37:00.376682Z",
     "shell.execute_reply": "2024-12-16T17:37:00.375283Z",
     "shell.execute_reply.started": "2024-12-16T17:37:00.320561Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Здание спортивного клуба, описанное в документе, представляет собой уникальное сооружение, сочетающее в себе элементы спортивного и культурного назначения. Оно расположено рядом с прудом и является ключевой частью архитектурной среды Екатеринбурга.\\n\\nАрхитектура спортивного клуба характеризуется традиционным стилем авангардизма и концепта конструктивизма. Здание имеет высокие ярусные балконы, видоискатели в стиле \"иллюминаторы\", триплетные уровни (главный и две секции) и смотровую площадку. Внутренние помещения включают тренажёрные залы, раздевалки, душевые, массажные кабины, инвентарные помещения и оркестровую комнату на втором этаже. Кроме того, в подвале расположен тир.\\n\\nФасады спортивного клуба оштукатурены и окрашены в классическом стиле, что соответствует эстетике времени его постройки. Он имеет четыре этажа с выступающим первым и овальным южным фасадом, что делает его легко узнаваемым. \\n\\nТаким образом, здание спортивного клуба представляет собой уникальное сооружение, объединяющее спортивные и культурные функции, с привлекательным дизайном и удобствами, которые способствуют его значимости в архитектурной среде Екатеринбурга.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
